{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6aZu5D9Q_Wd",
    "outputId": "a5ea2cbe-a709-4107-9c99-ced565cae02e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /Users/dhruv/opt/anaconda3/lib/python3.8/site-packages (0.0.52)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/dhruv/opt/anaconda3/lib/python3.8/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /Users/dhruv/opt/anaconda3/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Requirement already satisfied: anyascii in /Users/dhruv/opt/anaconda3/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
      "Requirement already satisfied: sklearn in /Users/dhruv/opt/anaconda3/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/dhruv/opt/anaconda3/lib/python3.8/site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/dhruv/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/dhruv/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/dhruv/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.20.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/dhruv/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/dhruv/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/dhruv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install sklearn \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bsheUgJ3Q_Wg"
   },
   "outputs": [],
   "source": [
    "#! pip install bs4 # in case you don't have it installed\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0fbDNo5Q_Wg"
   },
   "source": [
    "## 1. Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We read the data directly from the link using the \"read_csv\" function of pandas. \"error_bad_lines=False\" is used to Drop any row that contains bad data.\n",
    "\n",
    "2. We use \"dropna\" to drop any row which contains empty/null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fk3_JyvyQ_Wh",
    "outputId": "c3529b3c-1268-417f-d682-b0620f4bd75e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "b'Skipping line 16148: expected 15 fields, saw 22\\nSkipping line 20100: expected 15 fields, saw 22\\nSkipping line 45178: expected 15 fields, saw 22\\nSkipping line 48700: expected 15 fields, saw 22\\nSkipping line 63331: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 86053: expected 15 fields, saw 22\\nSkipping line 88858: expected 15 fields, saw 22\\nSkipping line 115017: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 137366: expected 15 fields, saw 22\\nSkipping line 139110: expected 15 fields, saw 22\\nSkipping line 165540: expected 15 fields, saw 22\\nSkipping line 171813: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 203723: expected 15 fields, saw 22\\nSkipping line 209366: expected 15 fields, saw 22\\nSkipping line 211310: expected 15 fields, saw 22\\nSkipping line 246351: expected 15 fields, saw 22\\nSkipping line 252364: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 267003: expected 15 fields, saw 22\\nSkipping line 268957: expected 15 fields, saw 22\\nSkipping line 303336: expected 15 fields, saw 22\\nSkipping line 306021: expected 15 fields, saw 22\\nSkipping line 311569: expected 15 fields, saw 22\\nSkipping line 316767: expected 15 fields, saw 22\\nSkipping line 324009: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 359107: expected 15 fields, saw 22\\nSkipping line 368367: expected 15 fields, saw 22\\nSkipping line 381180: expected 15 fields, saw 22\\nSkipping line 390453: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 412243: expected 15 fields, saw 22\\nSkipping line 419342: expected 15 fields, saw 22\\nSkipping line 457388: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 459935: expected 15 fields, saw 22\\nSkipping line 460167: expected 15 fields, saw 22\\nSkipping line 466460: expected 15 fields, saw 22\\nSkipping line 500314: expected 15 fields, saw 22\\nSkipping line 500339: expected 15 fields, saw 22\\nSkipping line 505396: expected 15 fields, saw 22\\nSkipping line 507760: expected 15 fields, saw 22\\nSkipping line 513626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 527638: expected 15 fields, saw 22\\nSkipping line 534209: expected 15 fields, saw 22\\nSkipping line 535687: expected 15 fields, saw 22\\nSkipping line 547671: expected 15 fields, saw 22\\nSkipping line 549054: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 599929: expected 15 fields, saw 22\\nSkipping line 604776: expected 15 fields, saw 22\\nSkipping line 609937: expected 15 fields, saw 22\\nSkipping line 632059: expected 15 fields, saw 22\\nSkipping line 638546: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 665017: expected 15 fields, saw 22\\nSkipping line 677680: expected 15 fields, saw 22\\nSkipping line 684370: expected 15 fields, saw 22\\nSkipping line 720217: expected 15 fields, saw 29\\n'\n",
      "b'Skipping line 723240: expected 15 fields, saw 22\\nSkipping line 723433: expected 15 fields, saw 22\\nSkipping line 763891: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 800288: expected 15 fields, saw 22\\nSkipping line 802942: expected 15 fields, saw 22\\nSkipping line 803379: expected 15 fields, saw 22\\nSkipping line 805122: expected 15 fields, saw 22\\nSkipping line 821899: expected 15 fields, saw 22\\nSkipping line 831707: expected 15 fields, saw 22\\nSkipping line 842829: expected 15 fields, saw 22\\nSkipping line 843604: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 863904: expected 15 fields, saw 22\\nSkipping line 875655: expected 15 fields, saw 22\\nSkipping line 886796: expected 15 fields, saw 22\\nSkipping line 892299: expected 15 fields, saw 22\\nSkipping line 902518: expected 15 fields, saw 22\\nSkipping line 903079: expected 15 fields, saw 22\\nSkipping line 912678: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 932953: expected 15 fields, saw 22\\nSkipping line 936838: expected 15 fields, saw 22\\nSkipping line 937177: expected 15 fields, saw 22\\nSkipping line 947695: expected 15 fields, saw 22\\nSkipping line 960713: expected 15 fields, saw 22\\nSkipping line 965225: expected 15 fields, saw 22\\nSkipping line 980776: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 999318: expected 15 fields, saw 22\\nSkipping line 1007247: expected 15 fields, saw 22\\nSkipping line 1015987: expected 15 fields, saw 22\\nSkipping line 1018984: expected 15 fields, saw 22\\nSkipping line 1028671: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1063360: expected 15 fields, saw 22\\nSkipping line 1066195: expected 15 fields, saw 22\\nSkipping line 1066578: expected 15 fields, saw 22\\nSkipping line 1066869: expected 15 fields, saw 22\\nSkipping line 1068809: expected 15 fields, saw 22\\nSkipping line 1069505: expected 15 fields, saw 22\\nSkipping line 1087983: expected 15 fields, saw 22\\nSkipping line 1108184: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1118137: expected 15 fields, saw 22\\nSkipping line 1142723: expected 15 fields, saw 22\\nSkipping line 1152492: expected 15 fields, saw 22\\nSkipping line 1156947: expected 15 fields, saw 22\\nSkipping line 1172563: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1209254: expected 15 fields, saw 22\\nSkipping line 1212966: expected 15 fields, saw 22\\nSkipping line 1236533: expected 15 fields, saw 22\\nSkipping line 1237598: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1273825: expected 15 fields, saw 22\\nSkipping line 1277898: expected 15 fields, saw 22\\nSkipping line 1283654: expected 15 fields, saw 22\\nSkipping line 1286023: expected 15 fields, saw 22\\nSkipping line 1302038: expected 15 fields, saw 22\\nSkipping line 1305179: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1326022: expected 15 fields, saw 22\\nSkipping line 1338120: expected 15 fields, saw 22\\nSkipping line 1338503: expected 15 fields, saw 22\\nSkipping line 1338849: expected 15 fields, saw 22\\nSkipping line 1341513: expected 15 fields, saw 22\\nSkipping line 1346493: expected 15 fields, saw 22\\nSkipping line 1373127: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1389508: expected 15 fields, saw 22\\nSkipping line 1413951: expected 15 fields, saw 22\\nSkipping line 1433626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1442698: expected 15 fields, saw 22\\nSkipping line 1472982: expected 15 fields, saw 22\\nSkipping line 1482282: expected 15 fields, saw 22\\nSkipping line 1487808: expected 15 fields, saw 22\\nSkipping line 1500636: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1511479: expected 15 fields, saw 22\\nSkipping line 1532302: expected 15 fields, saw 22\\nSkipping line 1537952: expected 15 fields, saw 22\\nSkipping line 1539951: expected 15 fields, saw 22\\nSkipping line 1541020: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1594217: expected 15 fields, saw 22\\nSkipping line 1612264: expected 15 fields, saw 22\\nSkipping line 1615907: expected 15 fields, saw 22\\nSkipping line 1621859: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1653542: expected 15 fields, saw 22\\nSkipping line 1671537: expected 15 fields, saw 22\\nSkipping line 1672879: expected 15 fields, saw 22\\nSkipping line 1674523: expected 15 fields, saw 22\\nSkipping line 1677355: expected 15 fields, saw 22\\nSkipping line 1703907: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1713046: expected 15 fields, saw 22\\nSkipping line 1722982: expected 15 fields, saw 22\\nSkipping line 1727290: expected 15 fields, saw 22\\nSkipping line 1744482: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1803858: expected 15 fields, saw 22\\nSkipping line 1810069: expected 15 fields, saw 22\\nSkipping line 1829751: expected 15 fields, saw 22\\nSkipping line 1831699: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1863131: expected 15 fields, saw 22\\nSkipping line 1867917: expected 15 fields, saw 22\\nSkipping line 1874790: expected 15 fields, saw 22\\nSkipping line 1879952: expected 15 fields, saw 22\\nSkipping line 1880501: expected 15 fields, saw 22\\nSkipping line 1886655: expected 15 fields, saw 22\\nSkipping line 1887888: expected 15 fields, saw 22\\nSkipping line 1894286: expected 15 fields, saw 22\\nSkipping line 1895400: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1904040: expected 15 fields, saw 22\\nSkipping line 1907604: expected 15 fields, saw 22\\nSkipping line 1915739: expected 15 fields, saw 22\\nSkipping line 1921514: expected 15 fields, saw 22\\nSkipping line 1939428: expected 15 fields, saw 22\\nSkipping line 1944342: expected 15 fields, saw 22\\nSkipping line 1949699: expected 15 fields, saw 22\\nSkipping line 1961872: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1968846: expected 15 fields, saw 22\\nSkipping line 1999941: expected 15 fields, saw 22\\nSkipping line 2001492: expected 15 fields, saw 22\\nSkipping line 2011204: expected 15 fields, saw 22\\nSkipping line 2025295: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2041266: expected 15 fields, saw 22\\nSkipping line 2073314: expected 15 fields, saw 22\\nSkipping line 2080133: expected 15 fields, saw 22\\nSkipping line 2088521: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2103490: expected 15 fields, saw 22\\nSkipping line 2115278: expected 15 fields, saw 22\\nSkipping line 2153174: expected 15 fields, saw 22\\nSkipping line 2161731: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2165250: expected 15 fields, saw 22\\nSkipping line 2175132: expected 15 fields, saw 22\\nSkipping line 2206817: expected 15 fields, saw 22\\nSkipping line 2215848: expected 15 fields, saw 22\\nSkipping line 2223811: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2257265: expected 15 fields, saw 22\\nSkipping line 2259163: expected 15 fields, saw 22\\nSkipping line 2263291: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2301943: expected 15 fields, saw 22\\nSkipping line 2304371: expected 15 fields, saw 22\\nSkipping line 2306015: expected 15 fields, saw 22\\nSkipping line 2312186: expected 15 fields, saw 22\\nSkipping line 2314740: expected 15 fields, saw 22\\nSkipping line 2317754: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2383514: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2449763: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2589323: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2775036: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2935174: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3078830: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3123091: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3185533: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4150395: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4748401: expected 15 fields, saw 22\\n'\n"
     ]
    }
   ],
   "source": [
    "#Initial_Dataset = pd.read_csv(\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz\",error_bad_lines=False,sep=\"\\t\") \n",
    "Initial_Dataset = pd.read_csv(\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz\",error_bad_lines=False,sep=\"\\t\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c1yVZZzQApJX"
   },
   "outputs": [],
   "source": [
    "Initial_Dataset = Initial_Dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSgYojthQ_Wi"
   },
   "source": [
    "## 2. Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need two columns, ( 'review_body', 'star_rating' ), and thus we ignore all the other columns and use only these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "azlrZavDQ_Wi"
   },
   "outputs": [],
   "source": [
    "Initial_Dataset = Initial_Dataset[['review_body','star_rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su18dDx89tzR"
   },
   "source": [
    "### Three Sample Reviews and Ratings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzAVnR1S9tUd",
    "outputId": "0e739aca-321a-469f-83e1-c210857ba96e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  star_rating\n",
      "2292547  Bought these for my kid's Frozen birthday part...          3.0\n",
      "3188810  These work great, make the cutest little sandw...          5.0\n",
      "41726                              Holds bottles perfectly          5.0\n"
     ]
    }
   ],
   "source": [
    "print(Initial_Dataset.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR5mpICf-Lmf"
   },
   "source": [
    "## 3. Statistics of Ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "-8nrNYPw-T0B",
    "outputId": "485ef9f9-2acf-49df-b4be-fd815e6e970d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.874562e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.207322e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.286991e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating\n",
       "count  4.874562e+06\n",
       "mean   4.207322e+00\n",
       "std    1.286991e+00\n",
       "min    1.000000e+00\n",
       "25%    4.000000e+00\n",
       "50%    5.000000e+00\n",
       "75%    5.000000e+00\n",
       "max    5.000000e+00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Initial_Dataset[[\"star_rating\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Goqc2sp1CbVg"
   },
   "source": [
    "#### Reviews grouped by Star Rating received:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDm5aeYHB5jG",
    "outputId": "025f691a-0ff1-4b0d-b15e-161b11ec3b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating  review_body\n",
      "0          1.0       426852\n",
      "1          2.0       241931\n",
      "2          3.0       349533\n",
      "3          4.0       731693\n",
      "4          5.0      3124553\n"
     ]
    }
   ],
   "source": [
    "Agg_Data = Initial_Dataset.groupby([\"star_rating\"]).count()\n",
    "Agg_Data = Agg_Data.reset_index()\n",
    "print(Agg_Data)\n",
    "# bring count in review_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count for each class of reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xv4nL-wVHtF1",
    "outputId": "a8c3da45-2376-4fde-afc9-a3ca0d1a963e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral Reviews:  349533 , Positive Reviews:  3856246 , Negavtive Reviews:  668783 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Neutral_Reviews  = Initial_Dataset[Initial_Dataset['star_rating']==3]['star_rating'].count()\n",
    "Positive_Reviews = Initial_Dataset[Initial_Dataset['star_rating']>3]['star_rating'].count()\n",
    "Negative_Reviews = Initial_Dataset[Initial_Dataset['star_rating']<3]['star_rating'].count()\n",
    "print('Neutral Reviews: ', Neutral_Reviews,',','Positive Reviews: ',Positive_Reviews,',','Negavtive Reviews: ',Negative_Reviews,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0XxESJNQ_Wi"
   },
   "source": [
    "## 4. Labelling Reviews:\n",
    "### The reviews with rating 4,5 are labelled to be 1 and 1,2 are labelled as 0. Discard the reviews with rating 3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to consider \"Neutral Reviews\" and we'll be ignoring them in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "f3GN4lEUQ_Wj"
   },
   "outputs": [],
   "source": [
    "df = Initial_Dataset[Initial_Dataset['star_rating']!=3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll add the \"Label\" column. We'll do labelling in the following way:\n",
    "1. if star_rating > 3 = 1\n",
    "2. if star_rating < 3 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qyCTkeSQ_Wj",
    "outputId": "b6b1c5f0-f77a-44a8-e033-2b6b19ce6d38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-5e5934107249>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'] = df['star_rating'].apply(lambda x: 1 if x >3 else 0)\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df['star_rating'].apply(lambda x: 1 if x >3 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWDJprZcQ_Wk"
   },
   "source": [
    " ### We select 200000 reviews randomly with 100,000 positive and 100,000 negative reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the \"sample\" function we'll select 100,000 random values where label = 1 and 100,000 random values where label = 0.\n",
    "\n",
    "2. After selecting random values, we'll join them together to make a smaller data set which is ready for Data Cleaning and Pre-Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "M50Hnr2PQ_Wk"
   },
   "outputs": [],
   "source": [
    "positive_ht = df[df.label == 1].sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0L3NKci0Q_Wk"
   },
   "outputs": [],
   "source": [
    "negative_ht = df[df.label == 0].sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "c20TD6YYQ_Wl"
   },
   "outputs": [],
   "source": [
    "data_set = pd.concat([positive_ht,negative_ht])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7csQUxvKYkV"
   },
   "source": [
    "## Average length of string BEFORE cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Z3yzTSEHJT_O"
   },
   "outputs": [],
   "source": [
    "temp1 = data_set[\"review_body\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SWtNHa3xKLpX",
    "outputId": "3d144b50-818e-48bc-b100-1a039eb5c137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of string BEFORE cleaning:  322.38637 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Avg_len_BeforeCleaning = temp1.mean()\n",
    "print('Average length of string BEFORE cleaning: ',Avg_len_BeforeCleaning,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aW--C3zQNf3E"
   },
   "source": [
    "## Three sample readings BEFORE data cleaning and pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs6M-6BLNYPS",
    "outputId": "3cd6b8fc-4321-409d-ffab-e84ea28e1f14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  star_rating  label\n",
      "2352194                                  Grindsmart coffee          5.0      1\n",
      "413769   Trim, beautiful look and no plastic!!!  Makes ...          5.0      1\n",
      "3315622  We visiting some friends and they mentioned th...          5.0      1\n"
     ]
    }
   ],
   "source": [
    "print(data_set.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcLr6ceyQ_Wl"
   },
   "source": [
    "## 5. Data Cleaning\n",
    "\n",
    "### 5.a) Convert the all reviews into the lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"str.lower( )\" is used to convert all characters to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FldbrM41Q_Wl"
   },
   "outputs": [],
   "source": [
    "data_set['review_body'] = data_set['review_body'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q2wiqVKQ_Wl"
   },
   "source": [
    "### 5.b) remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ennSKOSOQ_Wl"
   },
   "source": [
    "### Removing HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"BeautifulSoup\" library is used to extract text from HTML/XML files. We use it here to retrieve just the relevant text and ignore HTML text present in the 'review_body' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6g01HZTQ_Wm",
    "outputId": "21546b33-e272-4774-8ee1-a235213073a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:332: MarkupResemblesLocatorWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_set['review_body']  = [BeautifulSoup(X).getText() for X in data_set['review_body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rOyXKbnQ_Wm"
   },
   "source": [
    "### Removing URL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We used regular expression ( \" http\\S+|www.\\S+ \" ) to filter-out any text which falls under the category of a URL.\n",
    "2. We \"replaced\" every URL with a an empty \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KQRoCj1OQ_Wm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-0b30f0c5afab>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_set['review_body'] = data_set['review_body'].str.replace('http\\S+|www.\\S+', '', case=False)\n"
     ]
    }
   ],
   "source": [
    "data_set['review_body'] = data_set['review_body'].str.replace('http\\S+|www.\\S+', '', case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_KZfBa-Q_Wm"
   },
   "source": [
    "### 5.c) remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expression r'[^a-zA-Z ]+' represents all strings that contain non-alphabetical characters and we replaced them with \"\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mXw_Y1i-Q_Wm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-55bd09950c89>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_set['review_body'] = data_set.review_body.str.replace(r'[^a-zA-Z ]+', '')\n"
     ]
    }
   ],
   "source": [
    "data_set['review_body'] = data_set.review_body.str.replace(r'[^a-zA-Z ]+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx6r6kLeQ_Wn"
   },
   "source": [
    "### 5.d) Remove the extra spaces between the words\n",
    "\n",
    "1. Used \"strip\" to remove the extra-spaces at the beginning and the end of a string.\n",
    "2. Used the regular expression ( '\\s+' ) to remove the extra space between the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AwXAPbBJQ_Wn"
   },
   "outputs": [],
   "source": [
    "data_set['review_body'] = data_set['review_body'].str.strip()\n",
    "data_set['review_body'] = data_set.review_body.str.replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxUxZfPtQ_Wn"
   },
   "source": [
    "### 5.e) perform contractions on the reviews.\n",
    "\n",
    "1. Used the contractions library to expand all contrations like we'll = we will, you'll = You will..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "L_Qvh7eqQ_Wn"
   },
   "outputs": [],
   "source": [
    "data_set['review_body'] = data_set['review_body'].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VYo23E6K-f1"
   },
   "source": [
    "### Average length of string AFTER cleaning OR Average length of string BEFORE Pre-processing :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SMDKNnGLNHu",
    "outputId": "e9bf1b8a-4577-455b-fe2a-6ac6356224d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307.027425\n"
     ]
    }
   ],
   "source": [
    "temp2 = data_set[\"review_body\"].str.len()\n",
    "Average_length_AFTER_cleaning = temp2.mean()\n",
    "print(Average_length_AFTER_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYPXwitkQ_Wn"
   },
   "source": [
    "## 6. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.a) Removing Stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed all the stopwords from the 'review_body' column using the \"stopwords\" list downloaded from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "MAkurriJQ_Wn",
    "outputId": "c333e4d2-ce8e-4a05-d0e7-089c1bde1e31"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDdiQkG_Q_Wn"
   },
   "source": [
    "### 6.b) perform lemmatization  \n",
    "\n",
    "1. Lemmatization is the step at which we reduce a word to it's base form. Eg. Studying => Study\n",
    "2. Wordnet is an large, freely and publicly available lexical database for the English language.\n",
    "3. We use WordNetLemmatizer() and call the lemmatize() function on each word of the string.\n",
    "4. The \"string_word_lemmetize\" function lemmitizes each string (row) passed to it by lemmitizing each word of that string.\n",
    "5. We used WhitespaceTokenizer() to extract the tokens from string of words without whitespaces etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6vq54-8lQ_Wn"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def string_word_lemmetize(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "RcJjQohpQ_Wo"
   },
   "outputs": [],
   "source": [
    "data_set['review_body'] = data_set.review_body.apply(string_word_lemmetize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"string_word_lemmetize\" function returns a list and inorder to convert it into a string, we use the join function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hzhmfgNkQ_Wo"
   },
   "outputs": [],
   "source": [
    "data_set['review_body'] = data_set['review_body'].str.join(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_bANhRDOjpj"
   },
   "source": [
    "## Three sample readings AFTER data cleaning and pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvvhQYR4Q_Wo",
    "outputId": "ccac913e-4f64-4b2a-d196-f2413f3d4d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  star_rating  label\n",
      "4110443  company claim glass break resistant industry w...          2.0      0\n",
      "292836   bought one marshall dept store exact style mod...          1.0      0\n",
      "4206954  I kettle year liked size temp gauge looking ke...          2.0      0\n"
     ]
    }
   ],
   "source": [
    "print(data_set[['review_body','star_rating','label']].sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Kg3lqCYQkwI"
   },
   "source": [
    "## Average length of string AFTER pre-processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxm2yCeNQaXy",
    "outputId": "5f8d83a7-410c-440f-e10f-beab07a80723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190.182485\n"
     ]
    }
   ],
   "source": [
    "temp3 = data_set[\"review_body\"].str.len()\n",
    "Average_length_after_preprocessing = temp3.mean()\n",
    "print(Average_length_after_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKUVhOQZQ_Wo"
   },
   "source": [
    "### DATASET Split 80-20\n",
    "\n",
    "We split the data into training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "si-0xj-8Q_Wo"
   },
   "outputs": [],
   "source": [
    "review_body_train,review_body_test,label_train,label_test = train_test_split(data_set[\"review_body\"], data_set[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuYJ80jgQ_Wo"
   },
   "source": [
    "## 7. TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TF-IDF stands for Term Frequency â€“ Inverse Document Frequency and we use it for feature extraction.\n",
    "2. The TF-IDF algorithm is implemented using TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "lMQceNbxQ_Wo"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We usefit_transform( ) method on our training data and transform( ) method on our test data\n",
    "2. We usefit_transform( ) on training data so that we can scale the training data and learn the scaling parameters of that data (Mean, Variance)\n",
    "3. The parameters found using the fit_transform( ) will be used by transform( ) when working on the testing set.\n",
    "4. If we apply fit_transform on testing data as well, then our model would calculate new mean and variance, defeating the purpose of the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "xmxLizBMQ_Wo"
   },
   "outputs": [],
   "source": [
    "review_body_train_final = vectorizer.fit_transform(review_body_train)\n",
    "review_body_test_final = vectorizer.transform(review_body_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the datset ready which includes features and labels for the reviews. We have to now just use the data and train different models and test their accuracy. We use 4 different models:\n",
    "\n",
    "1. Perceptron (imported from sklearn.linear_model)\n",
    "2. SVM (imported from sklearn.svm)\n",
    "3. Logistic Regression (imported from sklearn.linear_model)\n",
    "4. Multinomial Naive Bayes (imported from sklearn.naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------FINAL OUTPUTS---------------\n",
      "\n",
      "Neutral Reviews:  349533 , Positive Reviews:  3856246 , Negavtive Reviews:  668783 \n",
      "\n",
      "Average length of string BEFORE cleaning:  322.38637\n",
      "Average length of string AFTER cleaning:  307.027425\n",
      "Average length of string BEFORE pre-processing:  307.027425\n",
      "Average length of string AFTER cleaning and pre-processing:  190.182485 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n---------------FINAL OUTPUTS---------------\\n\")\n",
    "print('Neutral Reviews: ', Neutral_Reviews,',','Positive Reviews: ',Positive_Reviews,',','Negavtive Reviews: ',Negative_Reviews,'\\n')\n",
    "print('Average length of string BEFORE cleaning: ',Avg_len_BeforeCleaning)\n",
    "print('Average length of string AFTER cleaning: ',Average_length_AFTER_cleaning)\n",
    "print('Average length of string BEFORE pre-processing: ',Average_length_AFTER_cleaning)\n",
    "print('Average length of string AFTER cleaning and pre-processing: ',Average_length_after_preprocessing,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITbtXPavQ_Wo"
   },
   "source": [
    "## 7.a) Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zwzl5elXQ_Wo"
   },
   "outputs": [],
   "source": [
    "ppn = Perceptron()\n",
    "ppn.fit(review_body_train_final, label_train)\n",
    "pred_ = ppn.predict(review_body_test_final)\n",
    "pred2 = ppn.predict(review_body_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iT0UaQtMQ_Wp",
    "outputId": "b21927a4-8262-45cb-c23d-b219bb6918f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perceptron:\n",
      "Testing Data:  0.85, 0.84, 0.87, 0.86\n",
      "Training Data: 0.92, 0.91, 0.93, 0.92\n"
     ]
    }
   ],
   "source": [
    "print('\\nPerceptron:')\n",
    "print('Testing Data: ',\"%.2f, %.2f, %.2f, %.2f\" % (accuracy_score(label_test, pred_),precision_score(label_test, pred_), recall_score(label_test, pred_), f1_score(label_test, pred_))) \n",
    "print('Training Data:',\"%.2f, %.2f, %.2f, %.2f\" % (accuracy_score(label_train, pred2),precision_score(label_train, pred2), recall_score(label_train, pred2), f1_score(label_train, pred2))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGHX4HYvQ_Wp"
   },
   "source": [
    "## 7.b) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xCOOZIRW-MC",
    "outputId": "fdde9807-fc6c-4cbd-d6fe-d1cfaa50965a"
   },
   "outputs": [],
   "source": [
    "lsvc = LinearSVC()\n",
    "lsvc.fit(review_body_train_final, label_train)\n",
    "pred_SVM = lsvc.predict(review_body_test_final)\n",
    "pred2SVM = lsvc.predict(review_body_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM:\n",
      "Testing Data:  0.89, 0.90, 0.89, 0.89\n",
      "Training Data: 0.94, 0.94, 0.94, 0.94\n"
     ]
    }
   ],
   "source": [
    "print('\\nSVM:')\n",
    "print('Testing Data: ',\"%.2f, %.2f, %.2f, %.2f\" % (accuracy_score(label_test, pred_SVM),precision_score(label_test, pred_SVM), recall_score(label_test, pred_SVM), f1_score(label_test, pred_SVM))) \n",
    "print('Training Data:',\"%.2f, %.2f, %.2f, %.2f\" % (accuracy_score(label_train, pred2SVM),precision_score(label_train, pred2SVM), recall_score(label_train, pred2SVM), f1_score(label_train, pred2SVM))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX180Cx5Q_Wp"
   },
   "source": [
    "## 7.c) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08_r9W5UQ_Wp",
    "outputId": "0597d0f7-20bc-4fb4-f8a5-b4f21a365f69"
   },
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression(max_iter=1000)\n",
    "logisticRegr.fit(review_body_train_final, label_train)\n",
    "pred_LR = logisticRegr.predict(review_body_test_final)\n",
    "pred2_LR = logisticRegr.predict(review_body_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression:\n",
      "Testing Data:  0.90, 0.90, 0.89, 0.90\n",
      "Training Data: 0.91, 0.92, 0.91, 0.91\n"
     ]
    }
   ],
   "source": [
    "print('\\nLogistic Regression:')\n",
    "print('Testing Data: ',\"%.2f, %.2f, %.2f, %.2f\" % (accuracy_score(label_test, pred_LR),precision_score(label_test, pred_LR), recall_score(label_test, pred_LR), f1_score(label_test, pred_LR))) \n",
    "print('Training Data:',\"%.2f, %.2f, %.2f, %.2f\" % (accuracy_score(label_train, pred2_LR),precision_score(label_train, pred2_LR), recall_score(label_train, pred2_LR), f1_score(label_train, pred2_LR))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EL3hkRncQ_Wq"
   },
   "source": [
    "## 7.d) Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgcgUNLaQ_Wq",
    "outputId": "bae0ac88-b8df-4f34-9208-a395082eb79b"
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(review_body_train_final, label_train)\n",
    "pred_NB = nb.predict(review_body_test_final)\n",
    "pred2_NB = nb.predict(review_body_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes:\n",
      "Testing Data:  0.87, 0.89, 0.85, 0.87\n",
      "Training Data: 0.89, 0.90, 0.88, 0.89\n"
     ]
    }
   ],
   "source": [
    "print('\\nNaive Bayes:')\n",
    "print('Testing Data: ',\"%.2f, %.2f, %.2f, %.2f\" % (accuracy_score(label_test, pred_NB),precision_score(label_test, pred_NB), recall_score(label_test, pred_NB), f1_score(label_test, pred_NB))) \n",
    "print('Training Data:',\"%.2f, %.2f, %.2f, %.2f\" % (accuracy_score(label_train, pred2_NB),precision_score(label_train, pred2_NB), recall_score(label_train, pred2_NB), f1_score(label_train, pred2_NB))) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW1-CSCI544.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
